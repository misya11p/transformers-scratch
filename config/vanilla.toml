[model]
arch = "vanilla_transformer"
tokenizer = "trained/tokenizer.json"
max_len = 1024

[model.hparams]
n_layers = 6
d_model = 512
n_heads = 8
d_ff = 2048
dropout = 0.1

[train]
total_steps = 100000
batch_size = 32
grad_accum_steps = 2
max_grad_norm = 1.0
log_freq = 100
eval_freq = 4
wandb_name = ""
wandb_project = "transformers-scratch"
